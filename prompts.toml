[general_messages]

tip_correct_format = "Tip: Make sure to answer in the correct format"

[data_integration_questionnaire_generator]

system_message = "You are an expert data integration and gouvernance expert that can ask questions about data integration and gouvernance best practices"
# Change for different best practices
best_practices = """
1. No code/ Low Code
Capability: There are more and more tools that are emerging in market making it easier to do data integration between systems without writing any code and use out of box connectors. 
The vast array of connectors gives organization agility to integrate with systems. 
Some of the most popular connectors include snowflake and salesforce connectors besides database connectors. There are many organizations that are migrating from expensive ETL tools like Informatica Powercenter to such cloud based tools.
Enabler: Such ease in integration gives the capability makes it easy to consume data downstream for applications and apply reverse ETL.

Low code and no code tools can speed up the integration of multiple systems.

2. Integration with Data Catalog
Capability: Are data moves across multiple systems, there is an increasing need to capture the metadata such as data lineage and exported to data catalog. 
There is also increasing demand to visualize data lineage and discover underlying relationships to improve data literacy.

Enabler: Using both data catalogs and data lineage, organizations can ensure data accuracy, implement data governance, and manage business change.

Data Catalogs can help organisations to document and understand better their data, thus helping firms and companies to improved the consistency and accuracy of their data.

3. CDC (Change Data Capture)
Capability: Publish changes in data as and when changes occur. Think event instead of schedule to minimize data latency.

Enabler: Minimizing data latency often leads to providing more timely transparency/visibility to business processes and thereby taking timely business decisions.

Change data capture is ideal when you want the data to be kept up to date all the time. It is a powerful tool to keep data consistency and keep all data related systems synchronized.

4. Open Source Abstraction on Compute Engine

Since organization have compute engines such as snowflake and databaricks already in their tech stack, there is a need to either use those compute engines for ETL operations or use an abstraction later top of these compute engines. 
Tools such as dbt (data build tool) is an open-source command-line tool that helps data analysts and engineers build data models and implement data pipelines. 
Another open-source tool Apache SeaTunnel is among the top 10 Apache projects for data integration. Tools such as Pentaho and Talend also can be leveraged.

ETL tools are ideal for building data pipelines and convert data into more suitable formats for data analysis and reporting.

5. Unified Integration at scale

Capability: There is an emerging trend where there is a need to do integration with a single tool whether its bulk/batch data or real time or streaming data. 
Organizations have multiple tools for data integration where ETL us used for bulk loads, Kafka used for real time publish, Nifi for streamlining. Some of the tools are on-prem and there is trend to move these workloads and data integration capabilities on cloud. 

Enabler: Having a unified platform simplifies maintainability of the integration platform.

Unified Data Integration- Why the Whole is Greater Than the Sum
Top companies now recognize the need for a more unified integration approach that combines the right technologies and managed services to deliver a more consistent and reliable view of their data across disparate applications, ultimately driving measurable business results.
Today’s enterprise data environments can be a goldmine of insight or a quagmire of confusion depending on the company and their approach to data integration and data management. 
Many struggle to manage a complex web of cloud applications — ironically designed to alleviate the very problems they now face with data accessibility and timeliness of delivery.
"""

human_message = """Based on the best practices specified below, please generate 3 questions to help a customer on his journey to enforce these best practices.
The best practices section starts with ==== BEST PRACTICES START ==== and ends with ==== BEST PRACTICES END ====.

==== BEST PRACTICES START ====
{best_practices}
==== BEST PRACTICES END ====
"""

